{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# models used in this notebook\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# to select best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# preprocessing steps\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for combining preprocessing and fitting\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://www.dropbox.com/s/012c6y4gxsxss6y/ghz.csv?dl=1\", parse_dates=[\"date\"])\n",
    "data.permno = data.permno.astype(int)\n",
    "data['date'] = data.date.dt.to_period('M')\n",
    "data = data.sort_values(by=['permno', 'date']).reset_index(drop=True)\n",
    "data.ret *= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS with statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>ret</td>       <th>  R-squared:         </th>  <td>   0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   11.78</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 11 Sep 2022</td> <th>  Prob (F-statistic):</th>  <td>7.63e-06</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:04:57</td>     <th>  Log-Likelihood:    </th> <td>-1.1078e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>282598</td>      <th>  AIC:               </th>  <td>2.216e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>282595</td>      <th>  BIC:               </th>  <td>2.216e+06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.3470</td> <td>    0.030</td> <td>   45.338</td> <td> 0.000</td> <td>    1.289</td> <td>    1.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acc</th>       <td>    0.1735</td> <td>    0.282</td> <td>    0.616</td> <td> 0.538</td> <td>   -0.378</td> <td>    0.725</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>agr</th>       <td>   -0.3454</td> <td>    0.071</td> <td>   -4.836</td> <td> 0.000</td> <td>   -0.485</td> <td>   -0.205</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>481798.941</td> <th>  Durbin-Watson:     </th>    <td>   2.045</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>16139038607.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td>10.459</td>   <th>  Prob(JB):          </th>    <td>    0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>1173.552</td>  <th>  Cond. No.          </th>    <td>    12.4</td>    \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    ret   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     11.78\n",
       "Date:                Sun, 11 Sep 2022   Prob (F-statistic):           7.63e-06\n",
       "Time:                        12:04:57   Log-Likelihood:            -1.1078e+06\n",
       "No. Observations:              282598   AIC:                         2.216e+06\n",
       "Df Residuals:                  282595   BIC:                         2.216e+06\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.3470      0.030     45.338      0.000       1.289       1.405\n",
       "acc            0.1735      0.282      0.616      0.538      -0.378       0.725\n",
       "agr           -0.3454      0.071     -4.836      0.000      -0.485      -0.205\n",
       "==============================================================================\n",
       "Omnibus:                   481798.941   Durbin-Watson:                   2.045\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):      16139038607.161\n",
       "Skew:                          10.459   Prob(JB):                         0.00\n",
       "Kurtosis:                    1173.552   Cond. No.                         12.4\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smf.ols(\"ret ~ acc + agr\", data=data)\n",
    "result = model.fit()\n",
    "result.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept = 1.3469625469981983 , OLS betas = [ 0.17345459 -0.34536579]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "X = data[['acc', 'agr']]\n",
    "y = data['ret']\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "print(\"intercept =\", model.intercept_, \", OLS betas =\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.29523485])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newx = pd.DataFrame({\"acc\": [0.1], \"agr\": [0.2]})\n",
    "model.predict(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test\n",
    "\n",
    "In ML, models are evaluated by how well they predict out of sample.  Split the data into train and test sets.  Fit the model on the training set.  Evaluate on the test set.  The standard metric for evaluating performance is the out-of-sample $R^2$.\n",
    "\n",
    "The out-of-sample $R^2$ is 1 - (sum of squared errors) / (sum of squared deviations from mean).  The model can be worse than predicting the mean (the mean is calculated from ytest and the model doesn't use ytest), so the out-of-sample $R^2$ can be negative.\n",
    "\n",
    "It is common to randomly split into training and test sets.  But, asset return data has a time element, and we usually take the test set to be after the training set.  Here is an example.\n",
    "\n",
    "In the model.fit line, regression coefficients are computed on the training data.  \n",
    "\n",
    "In the model.score line, the same regression coefficients are applied to the X variables in the test data to get predictions.  These predictions are compared to the actual y data in the test data set, andhe $R^2$ of the predictions is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS out-of-sample R2 is -0.0027\n"
     ]
    }
   ],
   "source": [
    "train = data[data.date<\"2020\"]\n",
    "test = data[data.date>=\"2020\"]\n",
    "\n",
    "Xtrain = train[['acc', 'agr']]\n",
    "Xtest = test[['acc', 'agr']]\n",
    "\n",
    "ytrain = train.ret \n",
    "ytest = test.ret\n",
    "\n",
    "model.fit(Xtrain, ytrain)\n",
    "score = model.score(Xtest, ytest)\n",
    "print(f\"OLS out-of-sample R2 is {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "OLS minimizes the mean squared error.  Lasso is an example of \"penalized linear regression.\"  It chooses coefficients to minimize\n",
    "\n",
    "$$\\frac{1}{2}\\text{MSE} + \\text{penalty} \\times \\sum_{i=1}^n |\\beta_i|$$\n",
    "\n",
    "The penalty is a parameter you input.  It is called \"alpha\" (not the regression intercept).\n",
    "\n",
    "The penalty is an example of what is called a hyperparameter.  It is not fit from the data.  Instead, it is an input.  We will discuss how to choose it optimally.\n",
    "\n",
    "The larger the penalty, the smaller the estimated betas will be.  For large alpha, the estimated betas will be zeros.\n",
    "\n",
    "Lasso is a way to do \"automatic feature selection.\"  Features are variables used to predict.  Dropping variables with zero lasso betas is a reasonable thing to do in some settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  0.0001 , Lasso betas =  [ 0.15824011 -0.3442439 ]\n",
      "alpha =  0.001 , Lasso betas =  [ 0.02130976 -0.33414689]\n",
      "alpha =  0.01 , Lasso betas =  [ 0.         -0.24684358]\n",
      "alpha =  0.1 , Lasso betas =  [ 0. -0.]\n",
      "alpha =  1 , Lasso betas =  [ 0. -0.]\n"
     ]
    }
   ],
   "source": [
    "for alpha in (0.0001, 0.001, 0.01, 0.1, 1):\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(X,y)\n",
    "    print(\"alpha = \", alpha, \", Lasso betas = \", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test with lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso out-of-sample R2 with alpha=0.01 is -0.0027\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "model = Lasso(alpha=alpha)\n",
    "model.fit(Xtrain, ytrain)\n",
    "score = model.score(Xtest, ytest)\n",
    "print(f\"Lasso out-of-sample R2 with alpha={alpha:.2f} is {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "In ridge regression, the coefficients are chosen to minimize\n",
    "$$\\text{SSE} + \\text{penalty} \\times \\sum_{i=1}^n \\beta_i^2$$\n",
    "Again, the penalty is called \"alpha.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  10 , Ridge betas =  [ 0.17251647 -0.34523807]\n",
      "alpha =  100 , Ridge betas =  [ 0.16450115 -0.3440968 ]\n",
      "alpha =  1000 , Ridge betas =  [ 0.11197401 -0.33332903]\n",
      "alpha =  10000 , Ridge betas =  [ 0.02521629 -0.25615389]\n"
     ]
    }
   ],
   "source": [
    "for alpha in (10, 100, 1000, 10000):\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X,y)\n",
    "    print(\"alpha = \", alpha, \", Ridge betas = \", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge out-of-sample R2 with alpha=10,000 is -0.0026\n"
     ]
    }
   ],
   "source": [
    "alpha=10000\n",
    "model = Ridge(alpha=alpha)\n",
    "model.fit(Xtrain, ytrain)\n",
    "score = model.score(Xtest, ytest)\n",
    "print(f\"Ridge out-of-sample R2 with alpha={alpha:,.0f} is {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net\n",
    "\n",
    "An elastic net combines the lasso and ridge penalties.  It chooses coefficients to minimize\n",
    "$$\\frac{1}{2} \\text{MSE} + \\text{alpha} \\times \\text{l1\\_ratio} \\times \\sum_{i=1}^n |\\beta_i| + \n",
    "\\text{alpha} \\times (1 - \\text{l1\\_ratio}) \\sum_{i=1}^n \\beta_i^2$$\n",
    "\n",
    "The sum of absolute values penalty is called an $\\ell^1$ penalty.  The sum of squares is called an $\\ell^2$ penalty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic net out-of-sample R2 with alpha=1.000 and l1_ratio=0.500 is -0.0025\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "l1_ratio = 0.5\n",
    "model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "model.fit(Xtrain, ytrain)\n",
    "score = model.score(Xtest, ytest)\n",
    "print(f\"Elastic net out-of-sample R2 with alpha={alpha:.3f} and l1_ratio={l1_ratio:.3f} is {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An aside on creating grids\n",
    "\n",
    "linspace(a,b) creates a grid [x0=1, x1, ..., xn=b] with points equally spaced.\n",
    "\n",
    "The default base for logspace is 10.  logspace(a,b) creates a grid [x0=a, x1, ..., xn=b] with points equally spaced and returns [10^x0, 10^x1, ..., 10^xn].  So, it samples small numbers more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.01    25.0075  50.005   75.0025 100.    ]\n",
      "[  0.01   0.1    1.    10.   100.  ]\n"
     ]
    }
   ],
   "source": [
    "grid1 = np.linspace(0.01, 100, 5)\n",
    "grid2 = np.logspace(-2, 2, 5)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(grid1)\n",
    "print(grid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validate, and test\n",
    "\n",
    "To choose hyperparameters like alpha and the l1_ratio in the elastic net, we don't want to optimize the score on the test set.  The maximum score is a biased estimate of what the model will achieve on new data.  So, we split our data into three sets and do the following:\n",
    "\n",
    "* train on the train set\n",
    "* calculate scores on the validation set\n",
    "* choose the hyperparameters with the best score on the validation set and retrain on train+validation sets\n",
    "* test on the test set\n",
    "\n",
    "Usually, the train, validation, and test sets are chosen randomly, but it is customary in asset pricing to train on old data, validate on newer data, and test on the newest data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha is 0.0016\n",
      "best l1_ratio is 0.9990\n"
     ]
    }
   ],
   "source": [
    "train2 = data[data.date<\"2015\"]\n",
    "validate = data[(data.date>=\"2015\") & (data.date<\"2020\")]\n",
    "\n",
    "Xtrain2 = train2[['acc', 'agr']]\n",
    "Xvalidate = validate[['acc', 'agr']]\n",
    "\n",
    "ytrain2 = train2.ret \n",
    "yvalidate = validate.ret \n",
    "\n",
    "alphas = np.logspace(-4, 2, 11)   \n",
    "l1_ratios = np.linspace(0.001, 0.999, 11)\n",
    "\n",
    "dct = {}\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        model.fit(Xtrain2, ytrain2)\n",
    "        score = model.score(Xvalidate, yvalidate)\n",
    "        dct[(alpha, l1_ratio)] = score\n",
    "        \n",
    "best_params = max(dct, key=dct.get)\n",
    "print(f\"best alpha is {best_params[0]:.4f}\")\n",
    "print(f\"best l1_ratio is {best_params[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having gotten some idea of what the best parameters are, we can search again within a narrower range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha is 0.0020\n",
      "best l1_ratio is 0.9990\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-3, -2, 11)\n",
    "l1_ratios = np.linspace(l1_ratios[-2], l1_ratios[-1], 11)\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        model.fit(Xtrain2, ytrain2)\n",
    "        score = model.score(Xvalidate, yvalidate)\n",
    "        dct[(alpha, l1_ratio)] = score\n",
    "\n",
    "best_params = max(dct, key=dct.get)\n",
    "print(f\"best alpha is {best_params[0]:.4f}\")\n",
    "print(f\"best l1_ratio is {best_params[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-fit on full training set (train = train2+validate) and test on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic net out-of-sample R2 with alpha=0.002 and l1_ratio = 0.9990 is -0.0027\n"
     ]
    }
   ],
   "source": [
    "alpha = best_params[0]\n",
    "l1_ratio = best_params[1]\n",
    "model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "model.fit(Xtrain, ytrain)\n",
    "score = model.score(Xtest, ytest)\n",
    "print(f\"Elastic net out-of-sample R2 with alpha={alpha:.3f} and l1_ratio = {l1_ratio:.4f} is {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside on storing calculations from loops\n",
    "\n",
    "We stored the elastic net calculations in a dictionary.  Here are some other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nser.name = \"score\"\\nser = ser.reset_index()\\nser\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the examples simpler\n",
    "\n",
    "alphas = alphas[:3]\n",
    "l1_ratios = l1_ratios[:3] \n",
    "\n",
    "# # store in a list\n",
    "\n",
    "lst = []\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        model.fit(Xtrain2, ytrain2)\n",
    "        score = model.score(Xvalidate, yvalidate)\n",
    "        lst.append(score)\n",
    "\n",
    "# store in a data frame\n",
    "\n",
    "df = pd.DataFrame(dtype=float, columns=['alpha', 'l1_ratio', 'score'])\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        model.fit(Xtrain2, ytrain2)\n",
    "        score = model.score(Xvalidate, yvalidate)\n",
    "        df.loc[df.shape[0]+1] = [alpha, l1_ratio, score]\n",
    "\n",
    "# store in a series\n",
    "\n",
    "indx = pd.MultiIndex.from_tuples(zip(alphas, l1_ratios))\n",
    "ser = pd.Series(dtype=float, index=indx)\n",
    "ser.index.names = ['alpha', 'l1_ratio']\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        model.fit(Xtrain2, ytrain2)\n",
    "        score = model.score(Xvalidate, yvalidate)\n",
    "        ser[(alpha, l1_ratio)] = score\n",
    "\n",
    "\n",
    "# lst\n",
    "# df\n",
    "# ser\n",
    "\"\"\"\n",
    "ser.name = \"score\"\n",
    "ser = ser.reset_index()\n",
    "ser\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross validation\n",
    "\n",
    "Instead of one training and one validation set, k-fold cross validation uses k training and validation sets.  \n",
    "\n",
    "First, split the data into train and test (no validation).  \n",
    "\n",
    "Then run k-fold cross validation on the training set.  The default for GridSearchCV is k=5.  With k=5, the training set is split into five pieces randomly.  1 of the 5 is chosen to be the validation set and the other 4 are lumped together to be a training set.  For each set of hyperparameters, the model is trained on the training set and tested on the validation set and the scores are saved.  Then a different 1 of the 5 is chosen to be the validation set and the process is repeated.  After all 5 have been used as validation sets, we have 5 scores for each set of hyperparameters.  Average them.  The hyperparameters with the highest average are the best parameters.\n",
    "\n",
    "GridSearchCV has an optional argument \"refit\" with the default being True.  This means that the model is refit on the full training set (all k folds) using the best set of hyperparameters.  This means that we are ready to test the model on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters are {'alpha': 0.01, 'l1_ratio': 0.001}\n",
      "out of sample R2 is -0.0027107038872318245\n",
      "cross-validation results are\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_l1_ratio</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026511</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001, 'l1_ratio': 0.001}</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>2.683273e-04</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'alpha': 0.001, 'l1_ratio': 0.5}</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.702071e-04</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019346</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.999</td>\n",
       "      <td>{'alpha': 0.001, 'l1_ratio': 0.999}</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>2.722209e-04</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018990</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.01, 'l1_ratio': 0.001}</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>2.687337e-04</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019404</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'alpha': 0.01, 'l1_ratio': 0.5}</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>2.673045e-04</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.020374</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.999</td>\n",
       "      <td>{'alpha': 0.01, 'l1_ratio': 0.999}</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>2.579378e-04</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.019610</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.1, 'l1_ratio': 0.001}</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>1.898230e-04</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.017996</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'alpha': 0.1, 'l1_ratio': 0.5}</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>2.746470e-05</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.026294</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.999</td>\n",
       "      <td>{'alpha': 0.1, 'l1_ratio': 0.999}</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>-1.029057e-07</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.026511      0.005900         0.005127        0.003173       0.001   \n",
       "1       0.021482      0.004445         0.002610        0.002049       0.001   \n",
       "2       0.019346      0.002217         0.003396        0.002051       0.001   \n",
       "3       0.018990      0.001998         0.001998        0.002448        0.01   \n",
       "4       0.019404      0.002908         0.001419        0.001965        0.01   \n",
       "5       0.020374      0.000768         0.000000        0.000000        0.01   \n",
       "6       0.019610      0.000758         0.001000        0.001999         0.1   \n",
       "7       0.017996      0.002441         0.001994        0.002442         0.1   \n",
       "8       0.026294      0.011379         0.001404        0.001958         0.1   \n",
       "\n",
       "  param_l1_ratio                               params  split0_test_score  \\\n",
       "0          0.001  {'alpha': 0.001, 'l1_ratio': 0.001}           0.000304   \n",
       "1            0.5    {'alpha': 0.001, 'l1_ratio': 0.5}           0.000300   \n",
       "2          0.999  {'alpha': 0.001, 'l1_ratio': 0.999}           0.000296   \n",
       "3          0.001   {'alpha': 0.01, 'l1_ratio': 0.001}           0.000272   \n",
       "4            0.5     {'alpha': 0.01, 'l1_ratio': 0.5}           0.000241   \n",
       "5          0.999   {'alpha': 0.01, 'l1_ratio': 0.999}           0.000211   \n",
       "6          0.001    {'alpha': 0.1, 'l1_ratio': 0.001}           0.000094   \n",
       "7            0.5      {'alpha': 0.1, 'l1_ratio': 0.5}          -0.000183   \n",
       "8          0.999    {'alpha': 0.1, 'l1_ratio': 0.999}          -0.000183   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0       2.683273e-04           0.000198          -0.000193           0.000171   \n",
       "1       2.702071e-04           0.000201          -0.000198           0.000169   \n",
       "2       2.722209e-04           0.000204          -0.000205           0.000167   \n",
       "3       2.687337e-04           0.000207          -0.000168           0.000170   \n",
       "4       2.673045e-04           0.000211          -0.000162           0.000166   \n",
       "5       2.579378e-04           0.000201          -0.000148           0.000166   \n",
       "6       1.898230e-04           0.000130          -0.000064           0.000140   \n",
       "7       2.746470e-05          -0.000054          -0.000055           0.000002   \n",
       "8      -1.029057e-07          -0.000061          -0.000068          -0.000001   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.000150        0.000178                2  \n",
       "1         0.000148        0.000179                3  \n",
       "2         0.000147        0.000182                4  \n",
       "3         0.000150        0.000164                1  \n",
       "4         0.000145        0.000157                5  \n",
       "5         0.000138        0.000146                6  \n",
       "6         0.000098        0.000087                7  \n",
       "7        -0.000052        0.000073                8  \n",
       "8        -0.000063        0.000067                9  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_grid = np.logspace(-3, -1, 3)\n",
    "l1_ratio_grid = np.linspace(0.001, 0.999, 3)\n",
    "model = GridSearchCV(ElasticNet(), param_grid={\"alpha\": alpha_grid, \"l1_ratio\": l1_ratio_grid})\n",
    "model.fit(Xtrain, ytrain)\n",
    "score = model.score(Xtest, ytest)\n",
    "print(\"best parameters are\", model.best_params_)\n",
    "print(\"out of sample R2 is\", score)\n",
    "print(\"cross-validation results are\")\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers and skewness\n",
    "\n",
    "Any standardizations that we apply on the training set must be applied also on the test set and for any new observations for which we want to predict.  \n",
    "\n",
    "If we winsorize, for example, we have to store the procedure (which points get mapped where) so we can apply it to new data.\n",
    "\n",
    "Winsorizing and taking logs are common in academic statistical work, but there are other transformations commonly used in ML.  Quantile transformer maps a variable monotonically into a variable having the desired distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsns.kdeplot(data.agr, label=\"agr\")\\nsns.kdeplot(data.transformed_agr, label=\"transformed agr\")\\nplt.legend()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt = QuantileTransformer(output_distribution=\"normal\")\n",
    "\n",
    "# the fit_transform method needs a column vector\n",
    "agr = data.agr.to_numpy().reshape(-1,1)\n",
    "\n",
    "data['transformed_agr'] = qt.fit_transform(agr)\n",
    "\n",
    "# see what we did\n",
    "\n",
    "# data.sort_values(by=\"date\").head()\n",
    "\n",
    "\"\"\"\n",
    "sns.kdeplot(data.agr, label=\"agr\")\n",
    "sns.kdeplot(data.transformed_agr, label=\"transformed agr\")\n",
    "plt.legend()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Penalized regression penalizes all coefficients the same, so it is important that the input variables be on the same scale.  StandardScaler subtracts the mean and divides by the std dev, so the transformed variable has zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_agr = data.transformed_agr.to_numpy().reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "data['transformed2_agr'] = scaler.fit_transform(transformed_agr)\n",
    "\n",
    "# sns.kdeplot(data.transformed2_agr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "We don't want to apply QuantileTransformer and StandardScaler directly to test data.  That would create new transformations, based on the properties of the test data.  How would we create those transformations for new observations?\n",
    "\n",
    "Instead, we want to remember the transformations used on the training data and apply the same transformations to the test data (and to any new observations that arise).  For example, StandardScaler subtracts a number (the mean) and divides by a number (the std dev).  We want to subtract the same number and divide by the same number on the test data.\n",
    "\n",
    "Pipelines let us transform and fit in one step.  Then, when we score the model on the test data, the same transformation is applied to the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9c182eb2c6b77cc7a6e636d1b5173df89a0a80b42c041014ee844b6f22ab6ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
